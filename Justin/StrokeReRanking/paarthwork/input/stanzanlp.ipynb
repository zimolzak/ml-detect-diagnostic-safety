{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liberal-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sunrise-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 12:45:33 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | mimic   |\n",
      "| pos       | mimic   |\n",
      "| lemma     | mimic   |\n",
      "| depparse  | mimic   |\n",
      "| ner       | i2b2    |\n",
      "=======================\n",
      "\n",
      "2021-06-24 12:45:33 INFO: Use device: cpu\n",
      "2021-06-24 12:45:33 INFO: Loading: tokenize\n",
      "2021-06-24 12:45:33 INFO: Loading: pos\n",
      "2021-06-24 12:45:34 INFO: Loading: lemma\n",
      "2021-06-24 12:45:34 INFO: Loading: depparse\n",
      "2021-06-24 12:45:35 INFO: Loading: ner\n",
      "2021-06-24 12:45:38 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame columns:  StudyID,ReportText,TIUStandardTitle,TIUDocumentDefinition,TIUDocumentSID\n"
     ]
    }
   ],
   "source": [
    "notesdf = pd.read_excel('P:/ORD_Singh_201911038D/Justin/InpatNote.xls')\n",
    "nlp = stanza.Pipeline('en', package='mimic', processors={'ner' : 'i2b2'}, ner_batch_size=512)\n",
    "print(\"DataFrame columns: \", ','.join([x for x in notesdf.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alike-sending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2730\n",
      "2730\n"
     ]
    }
   ],
   "source": [
    "print(len(['_'.join(x) for x in zip(notesdf.StudyID.astype(str), notesdf.TIUDocumentSID.astype(str))]))\n",
    "print(len(np.unique(['_'.join(x) for x in zip(notesdf.StudyID.astype(str), notesdf.TIUDocumentSID.astype(str))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unsigned-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "notesdf.ReportText = notesdf.ReportText.str.replace(\"\\s+\", \" \", regex=True).str.replace(\"'\", \"\").str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recorded-wrist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM \t\t dizziness\n",
      "PROBLEM \t\t vomiting\n",
      "PROBLEM \t\t Vomitus\n",
      "PROBLEM \t\t associated cold sweats\n",
      "PROBLEM \t\t these symptoms\n",
      "Wall time: 29.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testdoc = nlp(notesdf.ReportText.iloc[0])\n",
    "for x in testdoc.entities[:5]:\n",
    "    print(x.type, '\\t\\t', x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "olympic-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12h 11min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "entities_by_studydocid = {'_'.join(x):[] for x in zip(notesdf.StudyID.astype(str), notesdf.TIUDocumentSID.astype(str))}\n",
    "\n",
    "for row in notesdf.itertuples():\n",
    "    text = row.ReportText\n",
    "    studydocid = f'{row.StudyID}_{row.TIUDocumentSID}'\n",
    "    entities = nlp(text).entities\n",
    "    entities_by_studydocid[studydocid] += [entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broadband-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"note_entities_by_studydocid.pkl\",'wb') as outfile:\n",
    "    pickle.dump(entities_by_studydocid, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "floral-vault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df:\n",
      "study_ids, ed_duration, ed_inp_delta, age_val, img_flags, con_flags, elix_score, mod_label\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Load the old data set for which we have labels\n",
    "structdf = pd.read_csv(\"input.csv\")\n",
    "structdf = structdf.drop('smk_flags_man', axis=1)\n",
    "structdf = structdf.drop('smk_flags_db', axis=1)\n",
    "structdf = structdf[structdf.mod_label != 'INVALID']\n",
    "print(f'Columns in df:\\n{\", \".join([x for x in structdf.columns])}')\n",
    "label_column = 'mod_label'\n",
    "feature_columns = list(structdf.columns)\n",
    "feature_columns.remove(label_column)\n",
    "feature_columns_cont = [\n",
    "    'ed_inp_delta'\n",
    "    ,'ed_duration'\n",
    "    ,'elix_score'\n",
    "    ,'age_val'\n",
    "]\n",
    "feature_columns_cat = [\n",
    "    'img_flags'\n",
    "    ,'con_flags'\n",
    "]\n",
    "\n",
    "structdf['ed_inp_delta'] = MinMaxScaler().fit_transform(structdf['ed_inp_delta'].values.reshape(-1,1))\n",
    "structdf['ed_duration'] = MinMaxScaler().fit_transform(structdf['ed_duration'].values.reshape(-1,1))\n",
    "structdf['elix_score'] = MinMaxScaler().fit_transform(structdf['elix_score'].values.reshape(-1,1))\n",
    "structdf['age_val'] = MinMaxScaler().fit_transform(structdf['age_val'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "duplicate-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "nonotesids = set()\n",
    "for ids in structdf.study_ids:\n",
    "    if str(ids) not in [x.split('_')[0] for x in entities_by_studydocid.keys()]:\n",
    "        nonotesids.add(ids)\n",
    "print(len(nonotesids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "chicken-bread",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retaineddf = structdf[~structdf.study_ids.isin(nonotesids)]\n",
    "retaineddf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sunset-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_for_studyid(entity_holder, studyid):\n",
    "    returned_list = []\n",
    "    studyid = str(studyid)\n",
    "    for key,values in entity_holder.items():\n",
    "        if studyid == key.split('_')[0]:\n",
    "            values = values[0]  #always a list of lists\n",
    "            for entity in values:\n",
    "                returned_list.append(entity.text.lower())\n",
    "    return returned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fourth-instrument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chest and sinus congestion',\n",
       " 'weakness',\n",
       " 'dizziness',\n",
       " 'cp',\n",
       " 'sob',\n",
       " 'blood pressure',\n",
       " 'pulse',\n",
       " 'respirations',\n",
       " 'temperature',\n",
       " 'spo2']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entities_for_studyid(entities_by_studydocid, 2003)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "junior-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chest and sinus congestion weakness dizziness cp sob blood pressure pulse respirations temperature spo2 pulse oximetry pain scale medications fall precaution measures falls fall precautions wt temp pulse resp pulse oximetry pain cough dizzy weakness cough mild sob some nasal congestion generally weak chest pain tightness falls focal weakness n v d stools and urine melena hemoptysis dizzy falls cough weakness motrin knee pain naproxen hypothyroidism snomed ct hearing loss icd unspecified gingival and periodontal disease icd hyperlipidemia snomed ct vitamin b deficiency osteoarthritis of knee snomed ct colonic polyps primary malignant neoplasm of prostate snomed ct paresthesias icd pain in joint involving shoulder region active outpatient medications cyanocobalamin inj inject 1000mcg active (1ml) anemia levothyroxine na (synthroid) thyroid replacement simvastatin naproxen pain simvastatin cholesterol 3) syringe tamsulosin hcl urination terbinafine hcl fungal infection non-va aspirin motrin mexican motrin pe nad vitals orthostatic i check pale cervical lymphadenopathy cta rales rhonchi wheezing mgr tender edema calf pain a little dizzy standing orthostatic dizzyness head movement sob o2 sats ekg acute changes troponin cbc chem ua cxr cough sinus congestion sinusitis bronchitis fatigue mild dizzyness augmentin fluids acute progressive symptoms ekg dizzy dizzy vomiting unsteady gait dizzy positive affect chest pain home oxygen headache rhinorrhea watery eyes change in bowel or bladder blood in urine stool fever nvd nausea cough sore throat nuchal rigidity abd pain rash ecg pa ua iv right wrist blood ns\n"
     ]
    }
   ],
   "source": [
    "studyidents = {x:[] for x in retaineddf.study_ids}\n",
    "entstrings = []\n",
    "for key in studyidents:\n",
    "    studyidents[key] += extract_entities_for_studyid(entities_by_studydocid, key)\n",
    "    #if studyidents[key] == []:\n",
    "    #    continue #skip adding to entstrings, we'll remove the key from studyidents shortly\n",
    "    entstrings += [' '.join(studyidents[key])]\n",
    "print(entstrings[0]) #should basically be a \"raw\" corpus where the corpus is made up of extracted text using StanzaNLP clinical model; CountVectorizer expects this format"
   ]
  },
  {
   "cell_type": "raw",
   "id": "delayed-trunk",
   "metadata": {},
   "source": [
    "for key,value in studyidents.copy().items():\n",
    "    if value == []:\n",
    "        print(key)\n",
    "        del studyidents[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "brave-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "endangered-symbol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pain | active | blood | icd | the | medications | pressure | of | ct | take | dizziness | tab | pulse | non | in | headache | cap | hcl | symptoms | va | chest | bp | acute | any | one | heart | dl | snomed | and | his | medication | chronic | your | weakness | diabetes | ekg | nausea | weight | changes | or | left | disease | outpatient | head | calcium | vomiting | fever | right | lisinopril | hypertension | glucose | sct | mouth | temp | signs | na | edema | iv | stroke | aspirin | vision | temperature | eval | disorder | breath | to | loss | nasal | other | capsule | score | issu | acetaminophen | cholesterol | mg | cough | urine | oximetry | total | oral | insulin | distress | infection | evaluation | labs | tablet | dizzy | abdominal | rate | sob | mild | vital | vitamin | atorvastatin | htn | shortness | assessment | skin | treatment | ear\n",
      "(192, 5226)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(entstrings)\n",
    "print(' | '.join(np.asarray(vectorizer.get_feature_names())[np.argsort(np.sum(X.toarray(), axis=0))[-100:]][::-1]))\n",
    "print(X.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "several-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['the', 'of', 'in', 'and', 'your', 'or', 'to'] #manually added just looking at the top 100 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "grand-payment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pain | active | blood | icd | medications | pressure | ct | take | dizziness | tab | pulse | non | headache | cap | hcl | symptoms | va | chest | bp | acute | any | one | dl | heart | snomed | his | medication | chronic | weakness | diabetes | nausea | ekg | changes | weight | disease | left | outpatient | head | calcium | fever | vomiting | right | lisinopril | hypertension | glucose | sct | mouth | temp | signs | na | edema | iv | vision | aspirin | stroke | temperature | eval | disorder | breath | nasal | loss | other | capsule | acetaminophen | score | issu | cholesterol | mg | cough | urine | oximetry | total | oral | insulin | distress | evaluation | infection | dizzy | labs | tablet | sob | rate | abdominal | mild | htn | shortness | vital | atorvastatin | vitamin | skin | assessment | ear | treatment | neck | exam | o2 | level | ha | by | chills\n",
      "(192, 5219)\n",
      "(217, 8)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(entstrings)\n",
    "print(' | '.join(np.asarray(vectorizer.get_feature_names())[np.argsort(np.sum(X.toarray(), axis=0))[-100:]][::-1]))\n",
    "print(X.toarray().shape)\n",
    "print(structdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "decimal-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average Test AUC ± σ (100 iterations): 0.624 ± 0.000\n",
      "Average Train AUC ± σ (100 iterations): 0.795 ± 0.000\n"
     ]
    }
   ],
   "source": [
    "test_aucs = []\n",
    "train_aucs = []\n",
    "test_reports = []\n",
    "train_reports = []\n",
    "for i in range(100):\n",
    "    eX_train, eX_test, ey_train, ey_test = train_test_split(retaineddf.drop(\"mod_label\", axis=1), \\\n",
    "                                                            retaineddf[\"mod_label\"], \\\n",
    "                                                            test_size=0.8, random_state=42)\n",
    "    model = LogisticRegression(solver='liblinear', penalty='l2', C=0.55, max_iter=1000, fit_intercept = True)\n",
    "    etest = eX_test.iloc[:,:]\n",
    "    etrain = eX_train.iloc[:,:]\n",
    "    model.fit(etrain, ey_train)\n",
    "    test_probas  = model.predict_proba(etest)\n",
    "    test_preds = model.predict(etest)\n",
    "    train_preds = model.predict(etrain)\n",
    "    train_probas = model.predict_proba(etrain)\n",
    "    testauc = roc_auc_score(ey_test, test_probas[:,1])\n",
    "    trainauc = roc_auc_score(ey_train, train_probas[:,1])\n",
    "    test_aucs += [testauc]\n",
    "    train_aucs += [trainauc]\n",
    "    test_reports += [classification_report(ey_test, test_preds, output_dict=True)]\n",
    "    train_reports += [classification_report(ey_train, train_preds, output_dict=True)]\n",
    "\n",
    "print(f\" Average Test AUC \\u00B1 \\u03C3 (100 iterations): \" +\\\n",
    "      f\"{np.average(test_aucs):0.3f} \\u00B1 {np.std(test_aucs):0.3f}\")\n",
    "print(f\"Average Train AUC \\u00B1 \\u03C3 (100 iterations): \" +\\\n",
    "      f\"{np.average(train_aucs):0.3f} \\u00B1 {np.std(train_aucs):0.3f}\")\n",
    "'''\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "for train_idx, test_idx in skf.split(retaineddf.drop(\"mod_label\", axis=1), retaineddf.mod_label):\n",
    "    X_train = retaineddf.drop(\"mod_label\", axis=1).values[train_idx]\n",
    "    y_train = retaineddf.mod_label.iloc[train_idx]\n",
    "    MinMaxScaler().fit_transform(structdf['ed_inp_delta'].values.reshape(-1,1)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(retaineddf.drop(\"mod_label\", axis=1).values[test_idx])\n",
    "    print(classification_report(retaineddf.mod_label.iloc[test_idx], y_pred))\n",
    "    print()\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "suburban-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average Test AUC ± σ (100 iterations): 0.515 ± 0.001\n",
      "Average Train AUC ± σ (100 iterations): 1.000 ± 0.000\n"
     ]
    }
   ],
   "source": [
    "test_aucs = []\n",
    "train_aucs = []\n",
    "test_reports = []\n",
    "train_reports = []\n",
    "for i in range(100):\n",
    "    eX_train, eX_test, ey_train, ey_test = train_test_split(pd.concat([retaineddf.reset_index().drop(\"mod_label\", axis=1), pd.DataFrame(X.toarray())], axis=1), \\\n",
    "                                                            retaineddf[\"mod_label\"], \\\n",
    "                                                            test_size=0.8, random_state=42)\n",
    "    model = LogisticRegression(solver='liblinear', penalty='l1', C=10, max_iter=1000, fit_intercept = False)\n",
    "    etest = eX_test.iloc[:,:]\n",
    "    etrain = eX_train.iloc[:,:]\n",
    "    model.fit(etrain, ey_train)\n",
    "    test_probas  = model.predict_proba(etest)\n",
    "    test_preds = model.predict(etest)\n",
    "    train_preds = model.predict(etrain)\n",
    "    train_probas = model.predict_proba(etrain)\n",
    "    testauc = roc_auc_score(ey_test, test_probas[:,1])\n",
    "    trainauc = roc_auc_score(ey_train, train_probas[:,1])\n",
    "    test_aucs += [testauc]\n",
    "    train_aucs += [trainauc]\n",
    "    test_reports += [classification_report(ey_test, test_preds, output_dict=True)]\n",
    "    train_reports += [classification_report(ey_train, train_preds, output_dict=True)]\n",
    "\n",
    "print(f\" Average Test AUC \\u00B1 \\u03C3 (100 iterations): \" +\\\n",
    "      f\"{np.average(test_aucs):0.3f} \\u00B1 {np.std(test_aucs):0.3f}\")\n",
    "print(f\"Average Train AUC \\u00B1 \\u03C3 (100 iterations): \" +\\\n",
    "      f\"{np.average(train_aucs):0.3f} \\u00B1 {np.std(train_aucs):0.3f}\")\n",
    "'''\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "for train_idx, test_idx in skf.split(retaineddf.drop(\"mod_label\", axis=1), retaineddf.mod_label):\n",
    "    entcounts_normalized = MinMaxScaler().fit_transform(X.toarray())\n",
    "    X_train = np.hstack((retaineddf.drop(\"mod_label\", axis=1).values[train_idx], entcounts_normalized[train_idx]))\n",
    "    y_train = retaineddf.mod_label.iloc[train_idx]\n",
    "    clf = LogisticRegression(C=1,max_iter=10000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(np.hstack((retaineddf.drop(\"mod_label\", axis=1).values[test_idx], entcounts_normalized[test_idx])))\n",
    "    print(classification_report(retaineddf.mod_label.iloc[test_idx], y_pred))\n",
    "    print()\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-madison",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
