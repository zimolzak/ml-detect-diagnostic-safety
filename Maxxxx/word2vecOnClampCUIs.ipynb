{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gentle-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "subtle-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cui data\n",
    "# os.chdir('P:\\ORD_Singh_201911038D\\Justin\\Stroke_Notes_13OCT21') #make sure we're in the right place\n",
    "DATA_PATH = \"P:\\ORD_Singh_201911038D\\Justin\\Stroke_Notes_13OCT21\"\n",
    "with open(os.path.join(DATA_PATH, \"sids_to_clamp_cuis.json\"),'r') as infile:\n",
    "    cuidata = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "altered-grain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cuis', 'semtype', 'presence'])\n",
      "['C0008031', 'C1507320', 'C0015031', 'C4718442', 'C2707412', 'C0281822', 'C0043250', 'C0281822', 'C0398266', 'C3244243']\n"
     ]
    }
   ],
   "source": [
    "#print(list(cuidata)[0]) # print the first document ID\n",
    "print(cuidata[list(cuidata)[0]].keys()) # Just to show the data fields in each document\n",
    "print(cuidata[list(cuidata)[0]]['cuis'][:10]) # show the first 10 cuis of this specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "heard-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 29865/29865 [00:34<00:00, 866.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [1, 2], [2, 3]]\n",
      "3526722\n",
      "111303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We will get training pairs. One is the context term the next is the target term.\n",
    "trainingpairs = []\n",
    "vocab = dict()\n",
    "frequency = Counter()\n",
    "idx = 0\n",
    "\n",
    "# This could absolutely be accomplished in an alternative / faster way but it's sufficiently fast for now\n",
    "for doc in tqdm(cuidata):\n",
    "    cuis = cuidata[doc]['cuis']\n",
    "    cuiids = []\n",
    "    for x in cuis:\n",
    "        if x not in vocab.keys():\n",
    "            vocab.update({x:idx})\n",
    "            idx+=1\n",
    "        cuiids += [vocab[x]]\n",
    "    for i in range(len(cuiids)-1):\n",
    "        pair = cuiids[i:i+2]\n",
    "        if len(pair) < 2:\n",
    "            continue\n",
    "        if pair[0] != pair[1]:\n",
    "            trainingpairs += [pair] #no self references\n",
    "            frequency.update(pair)\n",
    "print(trainingpairs[:3])\n",
    "print(len(trainingpairs))\n",
    "print(len(vocab))\n",
    "assert np.all(np.asarray([x for x in vocab.values()]) == np.arange(len(vocab)))\n",
    "# We'll use an embedding dimension of 50 to start\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legal-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = np.asarray([values for key,values in frequency.items()])**0.75\n",
    "frequency /= np.linalg.norm(frequency, ord=1)\n",
    "frequency = torch.from_numpy(frequency)\n",
    "samplingids = torch.arange(0, len(frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alike-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUIEmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CUIEmbeddingModel, self).__init__()\n",
    "        self.dim = embedding_dim\n",
    "        self.in_embeddings = torch.nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.out_embeddings = torch.nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        \n",
    "    def init(self):\n",
    "        self.in_embeddings.weight.data.uniform_(-0.5/self.dim, 0.5/self.dim) # scaled by dimensionality to control initial norm.\n",
    "        self.out_embeddings.weight.data.uniform(-0, 0) # all 0s\n",
    "        \n",
    "    def forward(self, inputs, targets, negatives):\n",
    "        inembed = self.in_embeddings(inputs)\n",
    "        outembed = self.out_embeddings(targets)\n",
    "        pos_score = torch.sum(torch.mul(inembed, outembed), dim=1)\n",
    "        pos_score = torch.nn.functional.logsigmoid(pos_score)\n",
    "        negembed = self.out_embeddings(negatives)\n",
    "        neg_score = torch.bmm(negembed, inembed.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.nn.functional.logsigmoid(-1*neg_score)\n",
    "        return -1 * (torch.sum(pos_score)+torch.sum(neg_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wanted-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = CUIEmbeddingModel(len(vocab), EMBEDDING_DIM)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "NEGSAMPLES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "loved-heaven",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1764it [00:01, 930.75it/s]                                                                                             \n"
     ]
    }
   ],
   "source": [
    "#Test batching (also gives you an idea of theoretical max throughput in a way)\n",
    "BATCH_SIZE=2000\n",
    "for batchidx in tqdm(np.arange(0, len(trainingpairs), BATCH_SIZE), total = len(trainingpairs)//BATCH_SIZE):\n",
    "    data = trainingpairs[batchidx:batchidx+BATCH_SIZE]\n",
    "    context_idxs = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "natural-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_embedding_search(cui):\n",
    "    #C0948008 = ischemic stroke\n",
    "    for i in zip(*torch.topk(torch.nn.functional.cosine_similarity(model.in_embeddings.weight.data[vocab[cui]].view(1,-1), \n",
    "                                                                   model.in_embeddings.weight.data), 10, largest=True)):\n",
    "        print(f'{i[0]:0.3f}\\t{cuitranslate[id2vocab[int(i[1])]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "assisted-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vocab = {value:key for key,value in vocab.items()}\n",
    "with open(os.path.join(DATA_PATH, \"cuitranslate.json\"),'r') as infile:\n",
    "    cuitranslate = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "copyrighted-convert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'C0008031'), (1, 'C1507320'), (2, 'C0015031'), (3, 'C4718442'), (4, 'C2707412'), (5, 'C0281822'), (6, 'C0043250'), (7, 'C0398266'), (8, 'C3244243'), (9, 'C0018802')]\n",
      "[('C0000005', '(131)I-Macroaggregated Albumin'), ('C0000039', '1,2-dipalmitoylphosphatidylcholine'), ('C0000052', '1,4-alpha-Glucan Branching Enzyme'), ('C0000074', '1-Alkyl-2-Acylphosphatidates'), ('C0000084', '1-Carboxyglutamic Acid'), ('C0000096', '1-Methyl-3-isobutylxanthine'), ('C0000097', '1-Methyl-4-phenyl-1,2,3,6-tetrahydropyridine'), ('C0000098', '1-Methyl-4-phenylpyridinium'), ('C0000102', '1-Naphthylamine'), ('C0000103', '1-Naphthylisothiocyanate')]\n",
      "(131)I-Macroaggregated Albumin\n"
     ]
    }
   ],
   "source": [
    "print(list(id2vocab.items())[0:10])\n",
    "print(list(cuitranslate.items())[0:10])\n",
    "print(cuitranslate[\"C0000005\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-brisbane",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingpairs = torch.LongTensor(trainingpairs)\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    shuffler = torch.randperm(trainingpairs.shape[0])\n",
    "    trainingpairs = trainingpairs[shuffler].view(trainingpairs.size())\n",
    "    with tqdm(np.arange(0, len(trainingpairs), BATCH_SIZE), desc=f'Epoch {epoch+1}', total = len(trainingpairs)//BATCH_SIZE) as progress: #goes one example at a time\n",
    "        for batchidx in progress:\n",
    "            data = trainingpairs[batchidx:batchidx+BATCH_SIZE]\n",
    "            inputs = torch.cat((data[:, 0], data[:,1])) #we'll go bidirectional; usually not done I suppose\n",
    "            targets = torch.cat((data[:, 1], data[:,0]))#doubles the batch size\n",
    "            negatives = samplingids[frequency.multinomial(num_samples=inputs.shape[0]*NEGSAMPLES, replacement=True)].reshape(inputs.shape[0], NEGSAMPLES)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward(inputs, targets, negatives)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress.set_postfix(loss=f\"{loss.item():3.3f}\")\n",
    "        losses.append(total_loss)\n",
    "        progress.set_postfix(loss=f\"{total_loss:3.3f}\")\n",
    "    nearest_embedding_search('C0948008')\n",
    "    \n",
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "excited-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, i in enumerate(losses):\n",
    "    if losses[_-1] < i: #if loss is always monitonically decreasing, should print 0 only\n",
    "        print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "built-screen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 29865/29865 [01:16<00:00, 391.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# ToDo: Build document vectors from CUI vector components\n",
    "docvectors = np.zeros((len(cuidata), EMBEDDING_DIM))\n",
    "for i,doc in tqdm(enumerate(cuidata), total=len(cuidata)):\n",
    "    for cui in cuidata[doc]['cuis']:\n",
    "        docvectors[i] += model.in_embeddings.weight.data[vocab[cui]].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "wound-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 29865/29865 [01:07<00:00, 439.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# ToDo: Build document vectors from CUI vector components\n",
    "docvectors = np.zeros((len(cuidata), EMBEDDING_DIM))\n",
    "for i,doc in tqdm(enumerate(cuidata), total=len(cuidata)):\n",
    "    tmpvecs = np.zeros(EMBEDDING_DIM)\n",
    "    for cui in cuidata[doc]['cuis']:\n",
    "        tmpvecs += model.in_embeddings.weight.data[vocab[cui]].numpy()\n",
    "    docvectors[i] = tmpvecs / len(cuidata[doc]['cuis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "medical-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('average_document_vectors_28_FEB_2022.json','w') as outfile:\n",
    "    json.dump({x:list(y) for x,y in zip(cuidata.keys(), docvectors)}, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "imposed-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cui_vectors_15_NOV_2021.json\",'w') as outfile:\n",
    "    json.dump({key:model.in_embeddings.weight.data[value].tolist() for key,value in vocab.items()}, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-poverty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "embedded-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cui_model_15_NOV_2021.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accepting-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To load the saved pytorch model:\n",
    "\n",
    "model = CUIEmbeddingModel(len(vocab), EMBEDDING_DIM) #vocab and embedding dim must match what the model was trained on\n",
    "model.load_state_dict(torch.load(cui_model_15_NOV_2021.pt))\n",
    "model.eval() #set to evaluation mode\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "binding-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000\tCVA ETIOLOGY HEMORRHAGIC ISCHEMIC\n",
      "0.847\tAcute Cerebrovascular Accidents\n",
      "0.836\tIschemic stroke\n",
      "0.829\tLeft hemiparesis\n",
      "0.829\tStructure of middle cerebral artery\n",
      "0.822\tThalamic infarction\n",
      "0.821\tTransient Ischemic Attack\n",
      "0.820\tEvaluation\n",
      "0.815\tAcute ischemic stroke subtype\n",
      "0.815\ttransient ischemic attack without residual deficits\n"
     ]
    }
   ],
   "source": [
    "nearest_embedding_search('C0742946')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "secret-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-transparency",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
